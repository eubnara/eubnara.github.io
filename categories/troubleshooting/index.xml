<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Troubleshooting on EUB's second brain</title><link>https://eubnara.github.io/categories/troubleshooting/</link><description>Recent content in Troubleshooting on EUB's second brain</description><generator>Hugo -- 0.152.2</generator><language>en-us</language><lastBuildDate>Sat, 21 Jun 2025 15:13:00 +0900</lastBuildDate><atom:link href="https://eubnara.github.io/categories/troubleshooting/index.xml" rel="self" type="application/rss+xml"/><item><title>Proxy repository for cargo/rust</title><link>https://eubnara.github.io/computer-science/rust/cargo_repository_proxy/</link><pubDate>Sat, 21 Jun 2025 15:13:00 +0900</pubDate><guid>https://eubnara.github.io/computer-science/rust/cargo_repository_proxy/</guid><description>&lt;h1 id="sonatype-nexus-community-edition-사용-예"&gt;Sonatype Nexus community edition 사용 예&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;3.77 이상의 버전을 써야 ce 버전에서도 cargo repository 를 쓸 수 있다. 단, 3.77 버전부터는 사용량 hard limit 이 존재하게 된다. 다음 명령으로 간단히 로컬에 구동&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;docker run -d -p 8081:8081 --name nexus sonatype/nexus3
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cargo-proxy&lt;/code&gt; 로 repository 를 만들었다고 가정, remote url 은 &lt;code&gt;https://index.crates.io&lt;/code&gt; 로 한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="nexus" loading="lazy" src="https://eubnara.github.io/images/computer-science/rust/cargo_repository_proxy/nexus.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;.cargo/config.toml&lt;/code&gt; 에 다음과 같이 내용 추가.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;❯ cat .cargo/config.toml
[registries.nexus]
index = &amp;#34;sparse+http://localhost:8081/repository/cargo-proxy/&amp;#34;
[registry]
default = &amp;#34;nexus&amp;#34;
[source.crates-io]
replace-with = &amp;#34;nexus&amp;#34;
[source.nexus]
registry = &amp;#34;sparse+http://localhost:8081/repository/cargo-proxy/&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;a href="https://index.crates.io/"&gt;https://index.crates.io/&lt;/a&gt; 에 써있는 설명을 보니 &lt;code&gt;sparse&lt;/code&gt; 프로토콜을 쓰는 것 같다. 위에 config.toml 에서도 &lt;code&gt;sparse+&lt;/code&gt; 를 빼먹으면 정상동작하지 않는다.
&lt;img alt="index_crates_io" loading="lazy" src="https://eubnara.github.io/images/computer-science/rust/cargo_repository_proxy/index_crates_io.png"&gt;&lt;/p&gt;</description></item><item><title>Don't sync privileges from ranger to hive</title><link>https://eubnara.github.io/computer-science/hadoop/dont-sync-privileges-from-ranger-to-hive/</link><pubDate>Sat, 19 Oct 2024 16:26:00 +0900</pubDate><guid>https://eubnara.github.io/computer-science/hadoop/dont-sync-privileges-from-ranger-to-hive/</guid><description>&lt;p&gt;When Apache Ranger is configured for authorization on Secure Hadoop cluster, Hive below 4.x.x synchronizes all the ranger hive policies to rdbms for Hive as default. It is unnecessary and make unnecessary high load on db. See &lt;a href="https://issues.apache.org/jira/browse/HIVE-25391"&gt;https://issues.apache.org/jira/browse/HIVE-25391&lt;/a&gt;. You can disable it with the configurations on HiveServer2 as follows.&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;hive.privilege.synchronizer=false
&lt;/code&gt;&lt;/pre&gt;</description></item><item><title>Monitoring metrics related to "jute.maxbuffer"</title><link>https://eubnara.github.io/computer-science/hadoop/monitoring-jute.maxbuffer/</link><pubDate>Sat, 25 May 2024 22:34:00 +0900</pubDate><guid>https://eubnara.github.io/computer-science/hadoop/monitoring-jute.maxbuffer/</guid><description>&lt;p&gt;There is a configuration named as &amp;ldquo;jute.maxbuffer&amp;rdquo; when using zookeeper. This can be set on zookeeper client side or server side. On zookeeper client side, the setting should be lower than that on zookeeper server.
If a client gets data bigger than the setting, it will get an error.&lt;/p&gt;
&lt;p&gt;There are some related issue.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://issues.apache.org/jira/browse/HIVE-21993"&gt;https://issues.apache.org/jira/browse/HIVE-21993&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://issues.apache.org/jira/browse/YARN-2962"&gt;https://issues.apache.org/jira/browse/YARN-2962&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to avoid this errors. Some metrics should be monitored on zookeeper.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;last_client_response_size&lt;/code&gt; or &lt;code&gt;max_client_response_size&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;client_response_size&lt;/code&gt; is a response size in bytes from zookeeper server to client.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;last_proposal_size&lt;/code&gt; or &lt;code&gt;max_proposal_size&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;proposal_size&lt;/code&gt; is a proposal size in bytes from zookeeper server leader to follower.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;proposal&lt;/code&gt;?: refer to &lt;a href="https://zookeeper.apache.org/doc/r3.7.1/zookeeperInternals.html"&gt;https://zookeeper.apache.org/doc/r3.7.1/zookeeperInternals.html&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These values should be lower than &lt;code&gt;jute.maxbuffer&lt;/code&gt;. This setting can be set through jvm argument like &lt;code&gt;-Djute.maxbuffer=10485760&lt;/code&gt; (10mb).&lt;/p&gt;</description></item><item><title>Checklist for hive metastore when using mysql</title><link>https://eubnara.github.io/computer-science/hadoop/hivemetastore-mysql/</link><pubDate>Thu, 12 Oct 2023 08:34:00 +0900</pubDate><guid>https://eubnara.github.io/computer-science/hadoop/hivemetastore-mysql/</guid><description>&lt;h1 id="mysql-index"&gt;MySQL Index&lt;/h1&gt;
&lt;p&gt;There are some expensive operations for hive metastore when accessing or storing metadatas on RDBMS.&lt;br&gt;
Here are some official hive patches for indexing.&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;-- HIVE-21063
CREATE UNIQUE INDEX `NOTIFICATION_LOG_EVENT_ID` ON NOTIFICATION_LOG (`EVENT_ID`) USING BTREE;
-- HIVE-21487
CREATE INDEX COMPLETED_COMPACTIONS_RES ON COMPLETED_COMPACTIONS (CC_DATABASE,CC_TABLE,CC_PARTITION);
-- HIVE-27165
DROP INDEX TAB_COL_STATS_IDX ON TAB_COL_STATS;
CREATE INDEX TAB_COL_STATS_IDX ON TAB_COL_STATS (DB_NAME, TABLE_NAME, COLUMN_NAME, CAT_NAME) USING BTREE;
DROP INDEX PCS_STATS_IDX ON PART_COL_STATS;
CREATE INDEX PCS_STATS_IDX ON PART_COL_STATS (DB_NAME,TABLE_NAME,COLUMN_NAME,PARTITION_NAME,CAT_NAME) USING BTREE;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When you upgrade your hive, there are some changes on tables in rdbms. You can find needed SQLs depending on version at &lt;a href="https://github.com/apache/hive/tree/master/standalone-metastore/metastore-server/src/main/sql/mysql"&gt;https://github.com/apache/hive/tree/master/standalone-metastore/metastore-server/src/main/sql/mysql&lt;/a&gt;.&lt;/p&gt;</description></item><item><title>Some cases where "rdns = false" in krb5.conf does not work in Hadoop ecosystem</title><link>https://eubnara.github.io/computer-science/hadoop/rdns-false-not-work/</link><pubDate>Sun, 02 Jul 2023 18:48:00 +0900</pubDate><guid>https://eubnara.github.io/computer-science/hadoop/rdns-false-not-work/</guid><description>&lt;p&gt;&lt;a href="https://web.mit.edu/kerberos/krb5-1.13/doc/admin/princ_dns.html"&gt;https://web.mit.edu/kerberos/krb5-1.13/doc/admin/princ_dns.html&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Operating system bugs may prevent a setting of rdns = false from disabling reverse DNS lookup. Some versions of GNU libc have a bug in getaddrinfo() that cause them to look up PTR records even when not required. MIT Kerberos releases krb5-1.10.2 and newer have a workaround for this problem, as does the krb5-1.9.x series as of release krb5-1.9.4.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are some cases where &amp;ldquo;rdns = false&amp;rdquo; in krb5.conf is not respected in Hadoop ecosystem. You can try to modify &lt;code&gt;/etc/hosts&lt;/code&gt; or register PTR records to fix this kind of issues.&lt;/p&gt;</description></item><item><title>Hadoop commands</title><link>https://eubnara.github.io/computer-science/hadoop/commands/</link><pubDate>Sun, 05 Feb 2023 17:02:26 +0900</pubDate><guid>https://eubnara.github.io/computer-science/hadoop/commands/</guid><description>&lt;h1 id="hdfs"&gt;HDFS&lt;/h1&gt;
&lt;h2 id="reconfigure-without-restart"&gt;Reconfigure without restart&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html"&gt;https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;configurable keys are limited without restart&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;$ hdfs dfsadmin -reconfig namenode nn1.example.com:8020 properties
Node [nn1.example.com:8020] Reconfigurable properties:
dfs.block.placement.ec.classname
dfs.block.replicator.classname
dfs.heartbeat.interval
dfs.image.parallel.load
dfs.namenode.avoid.read.slow.datanode
dfs.namenode.block-placement-policy.exclude-slow-nodes.enabled
dfs.namenode.heartbeat.recheck-interval
dfs.namenode.max.slowpeer.collect.nodes
dfs.namenode.replication.max-streams
dfs.namenode.replication.max-streams-hard-limit
dfs.namenode.replication.work.multiplier.per.iteration
dfs.storage.policy.satisfier.mode
fs.protected.directories
hadoop.caller.context.enabled
ipc.8020.backoff.enable
&lt;/code&gt;&lt;/pre&gt;</description></item><item><title>SPNEGO-enabled Hadoop DataNode misjudges kerberos "replay attack".</title><link>https://eubnara.github.io/computer-science/hadoop/spnego-request-is-a-replay/</link><pubDate>Sun, 05 Feb 2023 16:01:17 +0900</pubDate><guid>https://eubnara.github.io/computer-science/hadoop/spnego-request-is-a-replay/</guid><description>&lt;ul&gt;
&lt;li&gt;references
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.cloudera.com/cloudera-manager/7.5.5/security-troubleshooting/cm-security-troubleshooting.pdf"&gt;https://docs.cloudera.com/cloudera-manager/7.5.5/security-troubleshooting/cm-security-troubleshooting.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://search-guard.com/elasticsearch-kibana-kerberos/"&gt;https://search-guard.com/elasticsearch-kibana-kerberos/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I guess that this is caused by sharing the same kerberos keytab (&lt;code&gt;/etc/security/keytabs/spnego.service.keytab&lt;/code&gt;) and principal(&lt;code&gt;HTTP/_HOST@{REALM}&lt;/code&gt;) among Hadoop daemons (NameNode, DataNode, JournalNodes, ResourceManager, NodeManager &amp;hellip;). I assume that DataNode misjudges it is a replay attack in certain circumstances.&lt;/p&gt;
&lt;p&gt;Adding the following jvm system properties to Hadoop daemons will fix this issue as a workaround. It means java process will not use replay cache.&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;-Dsun.security.krb5.rcache=none
&lt;/code&gt;&lt;/pre&gt;</description></item></channel></rss>