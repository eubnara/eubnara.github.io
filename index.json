[{"content":"When Apache Ranger is configured for authorization on Secure Hadoop cluster, Hive below 4.x.x synchronizes all the ranger hive policies to rdbms for Hive as default. It is unnecessary and make unnecessary high load on db. See https://issues.apache.org/jira/browse/HIVE-25391. You can disable it with the configurations on HiveServer2 as follows.\nhive.privilege.synchronizer=false ","permalink":"https://eubnara.github.io/computer-science/hadoop/dont-sync-privileges-from-ranger-to-hive/","summary":"\u003cp\u003eWhen Apache Ranger is configured for authorization on Secure Hadoop cluster, Hive below 4.x.x synchronizes all the ranger hive policies to rdbms for Hive as default. It is unnecessary and make unnecessary high load on db. See \u003ca href=\"https://issues.apache.org/jira/browse/HIVE-25391\"\u003ehttps://issues.apache.org/jira/browse/HIVE-25391\u003c/a\u003e. You can disable it with the configurations on HiveServer2 as follows.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ehive.privilege.synchronizer=false\n\u003c/code\u003e\u003c/pre\u003e","title":"Don't sync privileges from ranger to hive"},{"content":"There is a configuration named as \u0026ldquo;jute.maxbuffer\u0026rdquo; when using zookeeper. This can be set on zookeeper client side or server side. On zookeeper client side, the setting should be lower than that on zookeeper server. If a client gets data bigger than the setting, it will get an error.\nThere are some related issue.\nhttps://issues.apache.org/jira/browse/HIVE-21993 https://issues.apache.org/jira/browse/YARN-2962 In order to avoid this errors. Some metrics should be monitored on zookeeper.\nlast_client_response_size or max_client_response_size client_response_size is a response size in bytes from zookeeper server to client. last_proposal_size or max_proposal_size proposal_size is a proposal size in bytes from zookeeper server leader to follower. proposal?: refer to https://zookeeper.apache.org/doc/r3.7.1/zookeeperInternals.html. These values should be lower than jute.maxbuffer. This setting can be set through jvm argument like -Djute.maxbuffer=10485760 (10mb).\n","permalink":"https://eubnara.github.io/computer-science/hadoop/monitoring-jute.maxbuffer/","summary":"\u003cp\u003eThere is a configuration named as \u0026ldquo;jute.maxbuffer\u0026rdquo; when using zookeeper. This can be set on zookeeper client side or server side. On zookeeper client side, the setting should be lower than that on zookeeper server.\nIf a client gets data bigger than the setting, it will get an error.\u003c/p\u003e\n\u003cp\u003eThere are some related issue.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/HIVE-21993\"\u003ehttps://issues.apache.org/jira/browse/HIVE-21993\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/YARN-2962\"\u003ehttps://issues.apache.org/jira/browse/YARN-2962\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn order to avoid this errors. Some metrics should be monitored on zookeeper.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003elast_client_response_size\u003c/code\u003e or \u003ccode\u003emax_client_response_size\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eclient_response_size\u003c/code\u003e is a response size in bytes from zookeeper server to client.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003elast_proposal_size\u003c/code\u003e or \u003ccode\u003emax_proposal_size\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eproposal_size\u003c/code\u003e is a proposal size in bytes from zookeeper server leader to follower.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eproposal\u003c/code\u003e?: refer to \u003ca href=\"https://zookeeper.apache.org/doc/r3.7.1/zookeeperInternals.html\"\u003ehttps://zookeeper.apache.org/doc/r3.7.1/zookeeperInternals.html\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese values should be lower than \u003ccode\u003ejute.maxbuffer\u003c/code\u003e. This setting can be set through jvm argument like \u003ccode\u003e-Djute.maxbuffer=10485760\u003c/code\u003e (10mb).\u003c/p\u003e","title":"Monitoring metrics related to \"jute.maxbuffer\""},{"content":"MySQL Index There are some expensive operations for hive metastore when accessing or storing metadatas on RDBMS.\nHere are some official hive patches for indexing.\n-- HIVE-21063 CREATE UNIQUE INDEX `NOTIFICATION_LOG_EVENT_ID` ON NOTIFICATION_LOG (`EVENT_ID`) USING BTREE; -- HIVE-21487 CREATE INDEX COMPLETED_COMPACTIONS_RES ON COMPLETED_COMPACTIONS (CC_DATABASE,CC_TABLE,CC_PARTITION); -- HIVE-27165 DROP INDEX TAB_COL_STATS_IDX ON TAB_COL_STATS; CREATE INDEX TAB_COL_STATS_IDX ON TAB_COL_STATS (DB_NAME, TABLE_NAME, COLUMN_NAME, CAT_NAME) USING BTREE; DROP INDEX PCS_STATS_IDX ON PART_COL_STATS; CREATE INDEX PCS_STATS_IDX ON PART_COL_STATS (DB_NAME,TABLE_NAME,COLUMN_NAME,PARTITION_NAME,CAT_NAME) USING BTREE; When you upgrade your hive, there are some changes on tables in rdbms. You can find needed SQLs depending on version at https://github.com/apache/hive/tree/master/standalone-metastore/metastore-server/src/main/sql/mysql.\nAdditionally, slow query has been found for column EVENT_TIME on table NOTIFICATION_LOG. There is no official patch related to this but I create an index as follows:\nCREATE INDEX `NOTIFICATION_LOG_EVENT_TIME` ON NOTIFICATION_LOG (`EVENT_TIME`) USING BTREE; When using MySQL 8.x Since MySQL \u0026gt;= 8.0, utf8mb4 and utf8mb4_0900_ai_ci are used as default character and collation.\nOn apache hive, there are some related patches but not fully resolved. (e.g. https://issues.apache.org/jira/browse/HIVE-18083)\nTherefore, if you want to use MySQL \u0026gt;= 8.0 with Hive 3.1.3, I advise you to set mysql connection options on jdbc url like this:\njdbc:mysql://{your_mysql_host}/{database}?characterEncoding=latin1\u0026amp;connectionCollation=latin1_bin Refer to:\nhttps://dev.mysql.com/doc/connector-j/8.1/en/connector-j-reference-charsets.html https://dev.mysql.com/doc/connector-j/8.1/en/connector-j-connp-props-session.html#cj-conn-prop_connectionCollation ","permalink":"https://eubnara.github.io/computer-science/hadoop/hivemetastore-mysql/","summary":"\u003ch1 id=\"mysql-index\"\u003eMySQL Index\u003c/h1\u003e\n\u003cp\u003eThere are some expensive operations for hive metastore when accessing or storing metadatas on RDBMS.\u003cbr\u003e\nHere are some official hive patches for indexing.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e-- HIVE-21063\nCREATE UNIQUE INDEX `NOTIFICATION_LOG_EVENT_ID` ON NOTIFICATION_LOG (`EVENT_ID`) USING BTREE;\n\n-- HIVE-21487\nCREATE INDEX COMPLETED_COMPACTIONS_RES ON COMPLETED_COMPACTIONS (CC_DATABASE,CC_TABLE,CC_PARTITION);\n\n-- HIVE-27165\nDROP INDEX TAB_COL_STATS_IDX ON TAB_COL_STATS;\nCREATE INDEX TAB_COL_STATS_IDX ON TAB_COL_STATS (DB_NAME, TABLE_NAME, COLUMN_NAME, CAT_NAME) USING BTREE;\nDROP INDEX PCS_STATS_IDX ON PART_COL_STATS;\nCREATE INDEX PCS_STATS_IDX ON PART_COL_STATS (DB_NAME,TABLE_NAME,COLUMN_NAME,PARTITION_NAME,CAT_NAME) USING BTREE;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eWhen you upgrade your hive, there are some changes on tables in rdbms. You can find needed SQLs depending on version at \u003ca href=\"https://github.com/apache/hive/tree/master/standalone-metastore/metastore-server/src/main/sql/mysql\"\u003ehttps://github.com/apache/hive/tree/master/standalone-metastore/metastore-server/src/main/sql/mysql\u003c/a\u003e.\u003c/p\u003e","title":"Checklist for hive metastore when using mysql"},{"content":"https://web.mit.edu/kerberos/krb5-1.13/doc/admin/princ_dns.html\nOperating system bugs may prevent a setting of rdns = false from disabling reverse DNS lookup. Some versions of GNU libc have a bug in getaddrinfo() that cause them to look up PTR records even when not required. MIT Kerberos releases krb5-1.10.2 and newer have a workaround for this problem, as does the krb5-1.9.x series as of release krb5-1.9.4.\nThere are some cases where \u0026ldquo;rdns = false\u0026rdquo; in krb5.conf is not respected in Hadoop ecosystem. You can try to modify /etc/hosts or register PTR records to fix this kind of issues.\n1. HiveMetaStoreClient https://github.com/apache/hive/blob/rel/release-3.1.3/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java#L246\nif (uriResolverHook != null) { metastoreURIArray.addAll(uriResolverHook.resolveURI(tmpUri)); } else { metastoreURIArray.add(new URI( tmpUri.getScheme(), tmpUri.getUserInfo(), HadoopThriftAuthBridge.getBridge().getCanonicalHostName(tmpUri.getHost()), tmpUri.getPort(), tmpUri.getPath(), tmpUri.getQuery(), tmpUri.getFragment() )); } There is some logic to resolve canonical hostname from metastore.thrift.uris or hive.metastore.uris. If the hostname resolved is not you wanted, you can 3 workarounds.\nModify /etc/hosts Register PTR record for HiveMetastore servers (Not sure) Try metastore.uri.resolver or hive.metastore.uri.resolver. https://issues.apache.org/jira/browse/HIVE-18347 2. KuduClient java lib https://issues.apache.org/jira/browse/KUDU-2032 https://issues.apache.org/jira/browse/KUDU-2096 https://issues.apache.org/jira/browse/KUDU-3415 Register PTR records or try to fix KUDU-3415.\n3. HBase client hbase.unsafe.client.kerberos.hostname.disable.reversedns=true Set the configuration above in hbase-site.xml.\nhttps://github.com/apache/hbase/blob/25455b6fe3cbd8f093fd9bc8c51a1bab95353a62/hbase-client/src/main/java/org/apache/hadoop/hbase/security/provider/GssSaslClientAuthenticationProvider.java#L48 https://hbase.apache.org/book.html#hbase_default_configurations References https://deview.kr/data/deview/session/attach/1500_T3_이영곤_대용량_멀티테넌트_시큐어_하둡_클러스터_운영_경험기.pdf ","permalink":"https://eubnara.github.io/computer-science/hadoop/rdns-false-not-work/","summary":"\u003cp\u003e\u003ca href=\"https://web.mit.edu/kerberos/krb5-1.13/doc/admin/princ_dns.html\"\u003ehttps://web.mit.edu/kerberos/krb5-1.13/doc/admin/princ_dns.html\u003c/a\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eOperating system bugs may prevent a setting of rdns = false from disabling reverse DNS lookup. Some versions of GNU libc have a bug in getaddrinfo() that cause them to look up PTR records even when not required. MIT Kerberos releases krb5-1.10.2 and newer have a workaround for this problem, as does the krb5-1.9.x series as of release krb5-1.9.4.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThere are some cases where \u0026ldquo;rdns = false\u0026rdquo; in krb5.conf is not respected in Hadoop ecosystem. You can try to modify \u003ccode\u003e/etc/hosts\u003c/code\u003e or register PTR records to fix this kind of issues.\u003c/p\u003e","title":"Some cases where \"rdns = false\" in krb5.conf does not work in Hadoop ecosystem"},{"content":"HDFS Reconfigure without restart https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html\nconfigurable keys are limited without restart $ hdfs dfsadmin -reconfig namenode nn1.example.com:8020 properties Node [nn1.example.com:8020] Reconfigurable properties: dfs.block.placement.ec.classname dfs.block.replicator.classname dfs.heartbeat.interval dfs.image.parallel.load dfs.namenode.avoid.read.slow.datanode dfs.namenode.block-placement-policy.exclude-slow-nodes.enabled dfs.namenode.heartbeat.recheck-interval dfs.namenode.max.slowpeer.collect.nodes dfs.namenode.replication.max-streams dfs.namenode.replication.max-streams-hard-limit dfs.namenode.replication.work.multiplier.per.iteration dfs.storage.policy.satisfier.mode fs.protected.directories hadoop.caller.context.enabled ipc.8020.backoff.enable ","permalink":"https://eubnara.github.io/computer-science/hadoop/commands/","summary":"\u003ch1 id=\"hdfs\"\u003eHDFS\u003c/h1\u003e\n\u003ch2 id=\"reconfigure-without-restart\"\u003eReconfigure without restart\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html\"\u003ehttps://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003econfigurable keys are limited without restart\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e$ hdfs dfsadmin -reconfig namenode nn1.example.com:8020 properties\nNode [nn1.example.com:8020] Reconfigurable properties:\ndfs.block.placement.ec.classname\ndfs.block.replicator.classname\ndfs.heartbeat.interval\ndfs.image.parallel.load\ndfs.namenode.avoid.read.slow.datanode\ndfs.namenode.block-placement-policy.exclude-slow-nodes.enabled\ndfs.namenode.heartbeat.recheck-interval\ndfs.namenode.max.slowpeer.collect.nodes\ndfs.namenode.replication.max-streams\ndfs.namenode.replication.max-streams-hard-limit\ndfs.namenode.replication.work.multiplier.per.iteration\ndfs.storage.policy.satisfier.mode\nfs.protected.directories\nhadoop.caller.context.enabled\nipc.8020.backoff.enable\n\u003c/code\u003e\u003c/pre\u003e","title":"Hadoop commands"},{"content":" https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/UnixShellGuide.html#HADOOP_CLASSPATH In Hadoop ecosystem, HADOOP_CLASSPATH environment variable is commonly used in many places. Hive is use this variable, too. I wonder that how the HADOOP_CLASSPATH variable is used in a script like beeline. I cannot find HADOOP_CLASSPATH variable in Hive source codes. I finally figure out that when executing beeline it uses hadoop jar command. (https://github.com/apache/hive/blob/rel/release-3.1.3/bin/ext/beeline.sh#L35) It uses RunJar.java where HADOOP_CLASSPATH is used to set CLASSPATH. (https://github.com/apache/hadoop/blob/rel/release-3.3.4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/RunJar.java#L347-L351)\nIf something in Hadoop ecosystem uses RunJar#main, it probably repect HADOOP_CLASSPATH environment variable.\n","permalink":"https://eubnara.github.io/computer-science/hadoop/hadoop-classpath/","summary":"\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/UnixShellGuide.html#HADOOP_CLASSPATH\"\u003ehttps://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/UnixShellGuide.html#HADOOP_CLASSPATH\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn Hadoop ecosystem, \u003ccode\u003eHADOOP_CLASSPATH\u003c/code\u003e environment variable is commonly used in many places. \u003ccode\u003eHive\u003c/code\u003e is use this variable, too.\nI wonder that how the \u003ccode\u003eHADOOP_CLASSPATH\u003c/code\u003e variable is used in a script like \u003ccode\u003ebeeline\u003c/code\u003e. I cannot find \u003ccode\u003eHADOOP_CLASSPATH\u003c/code\u003e variable in \u003ccode\u003eHive\u003c/code\u003e source codes.\nI finally figure out that when executing \u003ccode\u003ebeeline\u003c/code\u003e it uses \u003ccode\u003ehadoop jar\u003c/code\u003e command. (\u003ca href=\"https://github.com/apache/hive/blob/rel/release-3.1.3/bin/ext/beeline.sh#L35\"\u003ehttps://github.com/apache/hive/blob/rel/release-3.1.3/bin/ext/beeline.sh#L35\u003c/a\u003e)\nIt uses \u003ccode\u003eRunJar.java\u003c/code\u003e where \u003ccode\u003eHADOOP_CLASSPATH\u003c/code\u003e is used to set \u003ccode\u003eCLASSPATH\u003c/code\u003e. (\u003ca href=\"https://github.com/apache/hadoop/blob/rel/release-3.3.4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/RunJar.java#L347-L351\"\u003ehttps://github.com/apache/hadoop/blob/rel/release-3.3.4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/RunJar.java#L347-L351\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003eIf something in Hadoop ecosystem uses \u003ccode\u003eRunJar#main\u003c/code\u003e, it probably repect \u003ccode\u003eHADOOP_CLASSPATH\u003c/code\u003e environment variable.\u003c/p\u003e","title":"About \"HADOOP_CLASSPATH\" environment variable"},{"content":" references https://docs.cloudera.com/cloudera-manager/7.5.5/security-troubleshooting/cm-security-troubleshooting.pdf https://search-guard.com/elasticsearch-kibana-kerberos/ I guess that this is caused by sharing the same kerberos keytab (/etc/security/keytabs/spnego.service.keytab) and principal(HTTP/_HOST@{REALM}) among Hadoop daemons (NameNode, DataNode, JournalNodes, ResourceManager, NodeManager \u0026hellip;). I assume that DataNode misjudges it is a replay attack in certain circumstances.\nAdding the following jvm system properties to Hadoop daemons will fix this issue as a workaround. It means java process will not use replay cache.\n-Dsun.security.krb5.rcache=none ","permalink":"https://eubnara.github.io/computer-science/hadoop/spnego-request-is-a-replay/","summary":"\u003cul\u003e\n\u003cli\u003ereferences\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://docs.cloudera.com/cloudera-manager/7.5.5/security-troubleshooting/cm-security-troubleshooting.pdf\"\u003ehttps://docs.cloudera.com/cloudera-manager/7.5.5/security-troubleshooting/cm-security-troubleshooting.pdf\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://search-guard.com/elasticsearch-kibana-kerberos/\"\u003ehttps://search-guard.com/elasticsearch-kibana-kerberos/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI guess that this is caused by sharing the same kerberos keytab (\u003ccode\u003e/etc/security/keytabs/spnego.service.keytab\u003c/code\u003e) and principal(\u003ccode\u003eHTTP/_HOST@{REALM}\u003c/code\u003e) among Hadoop daemons (NameNode, DataNode, JournalNodes, ResourceManager, NodeManager \u0026hellip;). I assume that DataNode misjudges it is a replay attack in certain circumstances.\u003c/p\u003e\n\u003cp\u003eAdding the following jvm system properties to Hadoop daemons will fix this issue as a workaround. It means java process will not use replay cache.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e-Dsun.security.krb5.rcache=none\n\u003c/code\u003e\u003c/pre\u003e","title":"SPNEGO-enabled Hadoop DataNode misjudges kerberos \"replay attack\"."},{"content":"My name is Yubi Lee.\nI work as a Data Engineer.\nemail: eubnara@gmail.com github: https://github.com/eubnara linkedin: https://kr.linkedin.com/in/yubi-lee-40a158130 Open source contribution Hadoop HDFS-17655: Cannot run HDFS balancer with BlockPlacementPolicyWithNodeGroup HADOOP-19319: configurationChangeMonitor is not properly set on HttpServer2 HADOOP-18806: Document missing property (ipc.server.read.threadpool.size) in core-default.xml HADOOP-18666: A whitelist of endpoints to skip Kerberos authentication doesn\u0026rsquo;t work for ResourceManager and Job History Server HDFS-16883: fix duplicate field name in hdfs-default.xml HADOOP-18585: DataNode\u0026rsquo;s internal infoserver redirects with http scheme, not https when https enabled. HADOOP-18398: Prevent AvroRecord*.class from being included non-test jar HADOOP-18087: fix bugs when looking up record from upstream DNS servers. When query A record which is chained by CNAME, YARN Registry DNS Server does not properly respond. Some CNAME records are missing. HADOOP-17861: improve YARN Registry DNS Server qps HDFS-13259: fix file preview bug in NameNode UI HDFS-13260: fix guide about HA with QJM Apache HBase HBASE-28911: Automatic SSL keystore reloading for HttpServer HBASE-28816: The description of \u0026ldquo;hbase.superuser\u0026rdquo; is confusing HBASE-28294: Support to skip Kerberos authentication for metric endpoints Apache Bigtop BIGTOP-3906: Wrapper script for hive sources wrong default file BIGTOP-3850: fix conflict when installing ranger-hdfs-plugin and ranger-yarn-plugin on the same machine Apache Ambari AMBARI-26075: Wrong datanode pid file when security enabled and unprivileged port is used AMBARI-25891: Enhancements when regenerating keytabs and changing Kerberos configurations AMBARI-25797: Fail to add a component on the same machine with ambari-server of a new service with no kerberos identity when kerberos enabled AMBARI-25788: Ambari server keeps generating keytabs even with KerberosServerAction#OperationType.CREATE_MISSING option. AMBARI-25624: optimize creating kerberos keytab branch-2.7: https://github.com/apache/ambari/pull/3353 trunk: https://github.com/apache/ambari/pull/3588 AMBARI-25720: Support Apache Directory Server AMBARI-25719: Fix bug when enabling kerberos in Ambari 2.7.6 trunk: https://github.com/apache/ambari/pull/3589 AMBARI-25619: improve \u0026ldquo;Prepare delete identities\u0026rdquo; process when deleting component in host in kerberized cluster AMBARI-25422: optimize loading the first page for Ambari UI AMBARI-25491: newline characters are ignored on custom property in Ambari Web editor Apache Ranger RANGER-4887: Change default configuration values for column masking and row-level filtering on hive policy RANGER-4886: Html-unescaping for user, group and role name on policy items RANGER-4418: Upgrade hadoop version and use shaded hadoop client artifacts RANGER-4247: auditPolicyEvaluators should be set before logging it RANGER-4236: enhance Ranger JSON audit to HDFS by compressing as gzip RANGER-4068: fix error caused by missing dnsjava library RANGER-3924: fix unnecessary sync caused by incremeting timestamp value typo RANGER-3858: On dev-support, service creation and ranger-kafka-plugin setup are failed Apache Spark SPARK-44976: Utils.getCurrentUserName should return the full principal name SPARK-40964: (TBD) Cannot run spark history server with shaded hadoop jar SPARK-40072: fix build failure when using make-distributions.sh Apache Impala IMPALA-9170: close idle connections without an associated session IMPALA-13327: bin/bootstrap_toolchain.py always download hbase-2.6.0-hadoop3-bin.tar.gz when USE_APACHE_COMPONENTS=true IMPALA-13311: Hive3 INSERT failed by ClassNotFoundException: org.apache.tez.runtime.api.Event https://gerrit.cloudera.org/#/c/21706/ IMPALA-13023: support webserver ldap filter when using spnego https://gerrit.cloudera.org/#/c/21339/ IMPALA-10408: Support Apache components to build Apache Impala to reduce dependencies of CDH or CDP https://gerrit.cloudera.org/#/c/18977/ Apache KUDU https://issues.apache.org/jira/browse/KUDU-3549: String gauge exposed in prometheus format Cloudera HUE https://github.com/cloudera/hue/pull/1134 (not merged): fix bug in handling + character in filebrowser https://github.com/cloudera/hue/pull/1135: fix bug in substituting liboozie.remote_deployement_dir variable Apache Zeppelin ZEPPELIN-5594: HDFS file id should be read as \u0026ldquo;long\u0026rdquo;, not \u0026ldquo;int\u0026rdquo;. Apache Avro AVRO-3877: [doc] fix wrong configuration for avro-maven-plugin in java example ETC Spring-shell https://github.com/spring-projects/spring-shell/pull/432#issuecomment-1144522860: add a feature to terminate spring-shell with CTRL+D gotty-client https://github.com/moul/gotty-client/pull/99: fix disconnection caused by not handling syscall.EINTR fix basic auth. token base64 encoding https://github.com/moul/gotty-client/pull/105 https://github.com/moul/gotty-client/pull/108 https://github.com/moul/gotty-client/pull/110: get password without revealing it react-native-cache https://github.com/timfpark/react-native-cache/pull/25: add a feature to support cache expiration discovery-zookeeper https://github.com/naver/discovery-zookeeper elasticsearch plugin to discovery hosts in cluster based on information in znode of Zookeeper winstonjs https://github.com/winstonjs/winston/pull/1559: fix confusing log message Presentation TossBank SLASH24: Building a Hadoop cluster with open source https://toss.im/slash-24/sessions/5 https://www.youtube.com/watch?v=4BcSVF4bLtU NAVER DEVIEW 2021: Large scale multi-tenant secure Hadoop cluster growth experience sharing https://deview.kr/2021/sessions/459 https://deview.kr/data/deview/session/attach/10_%EC%B4%88%EB%8C%80%EC%9A%A9%EB%9F%89%20%EB%A9%80%ED%8B%B0%ED%85%8C%EB%84%8C%ED%8A%B8%20%EC%8B%9C%ED%81%90%EC%96%B4%20%ED%95%98%EB%91%A1%20%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%20%EC%84%B1%EC%9E%A5%ED%86%B5%20%EA%B2%BD%ED%97%98%EA%B8%B0.pdf https://tv.naver.com/v/23650574 NAVER DEVIEW 2017: Advanced Experiences in Multi-tenant Hadoop Cluster Operation https://deview.kr/2017/schedule/193?lang=en https://www.slideshare.net/deview/234-80881396 https://tv.naver.com/v/2297124 ","permalink":"https://eubnara.github.io/about/","summary":"\u003cp\u003eMy name is Yubi Lee.\u003cbr\u003e\nI work as a Data Engineer.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eemail: \u003ca href=\"mailto:eubnara@gmail.com\"\u003eeubnara@gmail.com\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003egithub: \u003ca href=\"https://github.com/eubnara\"\u003ehttps://github.com/eubnara\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003elinkedin: \u003ca href=\"https://kr.linkedin.com/in/yubi-lee-40a158130\"\u003ehttps://kr.linkedin.com/in/yubi-lee-40a158130\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"open-source-contribution\"\u003eOpen source contribution\u003c/h1\u003e\n\u003ch2 id=\"hadoop\"\u003eHadoop\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/HDFS-17655\"\u003eHDFS-17655\u003c/a\u003e: Cannot run HDFS balancer with BlockPlacementPolicyWithNodeGroup\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/HADOOP-19319\"\u003eHADOOP-19319\u003c/a\u003e: configurationChangeMonitor is not properly set on HttpServer2\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/HADOOP-18806\"\u003eHADOOP-18806\u003c/a\u003e: Document missing property (ipc.server.read.threadpool.size) in core-default.xml\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/HADOOP-18666\"\u003eHADOOP-18666\u003c/a\u003e: A whitelist of endpoints to skip Kerberos authentication doesn\u0026rsquo;t work for ResourceManager and Job History Server\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/HDFS-16883\"\u003eHDFS-16883\u003c/a\u003e: fix duplicate field name in hdfs-default.xml\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/HADOOP-18585\"\u003eHADOOP-18585\u003c/a\u003e: DataNode\u0026rsquo;s internal infoserver redirects with http scheme, not https when https enabled.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/HADOOP-18398\"\u003eHADOOP-18398\u003c/a\u003e: Prevent AvroRecord*.class from being included non-test jar\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/HADOOP-18087\"\u003eHADOOP-18087\u003c/a\u003e: fix bugs when looking up record from upstream DNS servers.\n\u003cul\u003e\n\u003cli\u003eWhen query A record which is chained by CNAME, YARN Registry DNS Server does not properly respond. Some CNAME records are missing.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/HADOOP-17861\"\u003eHADOOP-17861\u003c/a\u003e: improve YARN Registry DNS Server qps\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/HDFS-13259\"\u003eHDFS-13259\u003c/a\u003e: fix file preview bug in NameNode UI\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/HDFS-13260\"\u003eHDFS-13260\u003c/a\u003e: fix guide about HA with QJM\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"apache-hbase\"\u003eApache HBase\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/HBASE-28911\"\u003eHBASE-28911\u003c/a\u003e: Automatic SSL keystore reloading for HttpServer\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/HBASE-28816\"\u003eHBASE-28816\u003c/a\u003e: The description of \u0026ldquo;hbase.superuser\u0026rdquo; is confusing\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/HBASE-28294\"\u003eHBASE-28294\u003c/a\u003e: Support to skip Kerberos authentication for metric endpoints\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"apache-bigtop\"\u003eApache Bigtop\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/BIGTOP-3906\"\u003eBIGTOP-3906\u003c/a\u003e: Wrapper script for hive sources wrong default file\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/BIGTOP-3850\"\u003eBIGTOP-3850\u003c/a\u003e: fix conflict when installing ranger-hdfs-plugin and ranger-yarn-plugin on the same machine\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"apache-ambari\"\u003eApache Ambari\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/AMBARI-26075\"\u003eAMBARI-26075\u003c/a\u003e: Wrong datanode pid file when security enabled and unprivileged port is used\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/AMBARI-25891\"\u003eAMBARI-25891\u003c/a\u003e: Enhancements when regenerating keytabs and changing Kerberos configurations\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/AMBARI-25797\"\u003eAMBARI-25797\u003c/a\u003e: Fail to add a component on the same machine with ambari-server of a new service with no kerberos identity when kerberos enabled\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/AMBARI-25788\"\u003eAMBARI-25788\u003c/a\u003e: Ambari server keeps generating keytabs even with KerberosServerAction#OperationType.CREATE_MISSING option.\u003c/li\u003e\n\u003cli\u003eAMBARI-25624: optimize creating kerberos keytab\n\u003cul\u003e\n\u003cli\u003ebranch-2.7: \u003ca href=\"https://github.com/apache/ambari/pull/3353\"\u003ehttps://github.com/apache/ambari/pull/3353\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003etrunk: \u003ca href=\"https://github.com/apache/ambari/pull/3588\"\u003ehttps://github.com/apache/ambari/pull/3588\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/AMBARI-25720\"\u003eAMBARI-25720\u003c/a\u003e: Support Apache Directory Server\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/AMBARI-25719\"\u003eAMBARI-25719\u003c/a\u003e: Fix bug when enabling kerberos in Ambari 2.7.6\n\u003cul\u003e\n\u003cli\u003etrunk: \u003ca href=\"https://github.com/apache/ambari/pull/3589\"\u003ehttps://github.com/apache/ambari/pull/3589\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/AMBARI-25619\"\u003eAMBARI-25619\u003c/a\u003e: improve \u0026ldquo;Prepare delete identities\u0026rdquo; process when deleting component in host in kerberized cluster\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/AMBARI-25422\"\u003eAMBARI-25422\u003c/a\u003e: optimize loading the first page for Ambari UI\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/AMBARI-25491\"\u003eAMBARI-25491\u003c/a\u003e: newline characters are ignored on custom property in Ambari Web editor\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"apache-ranger\"\u003eApache Ranger\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/RANGER-4887\"\u003eRANGER-4887\u003c/a\u003e: Change default configuration values for column masking and row-level filtering on hive policy\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/RANGER-4886\"\u003eRANGER-4886\u003c/a\u003e: Html-unescaping for user, group and role name on policy items\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/RANGER-4418\"\u003eRANGER-4418\u003c/a\u003e: Upgrade hadoop version and use shaded hadoop client artifacts\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/RANGER-4247\"\u003eRANGER-4247\u003c/a\u003e: auditPolicyEvaluators should be set before logging it\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/RANGER-4236\"\u003eRANGER-4236\u003c/a\u003e: enhance Ranger JSON audit to HDFS by compressing as gzip\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/RANGER-4068\"\u003eRANGER-4068\u003c/a\u003e: fix error caused by missing dnsjava library\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/RANGER-3924\"\u003eRANGER-3924\u003c/a\u003e: fix unnecessary sync caused by incremeting timestamp value typo\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/RANGER-3858\"\u003eRANGER-3858\u003c/a\u003e: On dev-support, service creation and ranger-kafka-plugin setup are failed\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"apache-spark\"\u003eApache Spark\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/SPARK-44976\"\u003eSPARK-44976\u003c/a\u003e: Utils.getCurrentUserName should return the full principal name\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/SPARK-40964\"\u003eSPARK-40964\u003c/a\u003e: (TBD) Cannot run spark history server with shaded hadoop jar\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/SPARK-40072\"\u003eSPARK-40072\u003c/a\u003e: fix build failure when using \u003ccode\u003emake-distributions.sh\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"apache-impala\"\u003eApache Impala\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/IMPALA-9170\"\u003eIMPALA-9170\u003c/a\u003e: close idle connections without an associated session\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/IMPALA-13327\"\u003eIMPALA-13327\u003c/a\u003e: bin/bootstrap_toolchain.py always download hbase-2.6.0-hadoop3-bin.tar.gz when USE_APACHE_COMPONENTS=true\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/IMPALA-13311\"\u003eIMPALA-13311\u003c/a\u003e: Hive3 INSERT failed by ClassNotFoundException: org.apache.tez.runtime.api.Event\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://gerrit.cloudera.org/#/c/21706/\"\u003ehttps://gerrit.cloudera.org/#/c/21706/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/IMPALA-13023\"\u003eIMPALA-13023\u003c/a\u003e: support webserver ldap filter when using spnego\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://gerrit.cloudera.org/#/c/21339/\"\u003ehttps://gerrit.cloudera.org/#/c/21339/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/IMPALA-10408\"\u003eIMPALA-10408\u003c/a\u003e: Support Apache components to build Apache Impala to reduce dependencies of CDH or CDP\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://gerrit.cloudera.org/#/c/18977/\"\u003ehttps://gerrit.cloudera.org/#/c/18977/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"apache-kudu\"\u003eApache KUDU\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/KUDU-3549\"\u003ehttps://issues.apache.org/jira/browse/KUDU-3549\u003c/a\u003e: String gauge exposed in prometheus format\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"cloudera-hue\"\u003eCloudera HUE\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/cloudera/hue/pull/1134\"\u003ehttps://github.com/cloudera/hue/pull/1134\u003c/a\u003e (not merged): fix bug in handling \u003ccode\u003e+\u003c/code\u003e character in filebrowser\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/cloudera/hue/pull/1135\"\u003ehttps://github.com/cloudera/hue/pull/1135\u003c/a\u003e: fix bug in substituting \u003ccode\u003eliboozie.remote_deployement_dir\u003c/code\u003e variable\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"apache-zeppelin\"\u003eApache Zeppelin\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/ZEPPELIN-5594\"\u003eZEPPELIN-5594\u003c/a\u003e: HDFS file id should be read as \u0026ldquo;long\u0026rdquo;, not \u0026ldquo;int\u0026rdquo;.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"apache-avro\"\u003eApache Avro\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://issues.apache.org/jira/browse/AVRO-3877\"\u003eAVRO-3877\u003c/a\u003e: [doc] fix wrong configuration for avro-maven-plugin in java example\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"etc\"\u003eETC\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eSpring-shell\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/spring-projects/spring-shell/pull/432#issuecomment-1144522860\"\u003ehttps://github.com/spring-projects/spring-shell/pull/432#issuecomment-1144522860\u003c/a\u003e: add a feature to terminate spring-shell with \u003ccode\u003eCTRL+D\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003egotty-client\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/moul/gotty-client/pull/99\"\u003ehttps://github.com/moul/gotty-client/pull/99\u003c/a\u003e: fix disconnection caused by not handling \u003ccode\u003esyscall.EINTR\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003efix basic auth. token base64 encoding\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/moul/gotty-client/pull/105\"\u003ehttps://github.com/moul/gotty-client/pull/105\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/moul/gotty-client/pull/108\"\u003ehttps://github.com/moul/gotty-client/pull/108\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/moul/gotty-client/pull/110\"\u003ehttps://github.com/moul/gotty-client/pull/110\u003c/a\u003e: get password without revealing it\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ereact-native-cache\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/timfpark/react-native-cache/pull/25\"\u003ehttps://github.com/timfpark/react-native-cache/pull/25\u003c/a\u003e: add a feature to support cache expiration\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ediscovery-zookeeper\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/naver/discovery-zookeeper\"\u003ehttps://github.com/naver/discovery-zookeeper\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eelasticsearch plugin to discovery hosts in cluster based on information in znode of Zookeeper\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ewinstonjs\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/winstonjs/winston/pull/1559\"\u003ehttps://github.com/winstonjs/winston/pull/1559\u003c/a\u003e: fix confusing log message\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"presentation\"\u003ePresentation\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eTossBank SLASH24: Building a Hadoop cluster with open source\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://toss.im/slash-24/sessions/5\"\u003ehttps://toss.im/slash-24/sessions/5\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/watch?v=4BcSVF4bLtU\"\u003ehttps://www.youtube.com/watch?v=4BcSVF4bLtU\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eNAVER DEVIEW 2021: Large scale multi-tenant secure Hadoop cluster growth experience sharing\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://deview.kr/2021/sessions/459\"\u003ehttps://deview.kr/2021/sessions/459\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://deview.kr/data/deview/session/attach/10_%EC%B4%88%EB%8C%80%EC%9A%A9%EB%9F%89%20%EB%A9%80%ED%8B%B0%ED%85%8C%EB%84%8C%ED%8A%B8%20%EC%8B%9C%ED%81%90%EC%96%B4%20%ED%95%98%EB%91%A1%20%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%20%EC%84%B1%EC%9E%A5%ED%86%B5%20%EA%B2%BD%ED%97%98%EA%B8%B0.pdf\"\u003ehttps://deview.kr/data/deview/session/attach/10_%EC%B4%88%EB%8C%80%EC%9A%A9%EB%9F%89%20%EB%A9%80%ED%8B%B0%ED%85%8C%EB%84%8C%ED%8A%B8%20%EC%8B%9C%ED%81%90%EC%96%B4%20%ED%95%98%EB%91%A1%20%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%20%EC%84%B1%EC%9E%A5%ED%86%B5%20%EA%B2%BD%ED%97%98%EA%B8%B0.pdf\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://tv.naver.com/v/23650574\"\u003ehttps://tv.naver.com/v/23650574\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eNAVER DEVIEW 2017: Advanced Experiences in Multi-tenant Hadoop Cluster Operation\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://deview.kr/2017/schedule/193?lang=en\"\u003ehttps://deview.kr/2017/schedule/193?lang=en\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.slideshare.net/deview/234-80881396\"\u003ehttps://www.slideshare.net/deview/234-80881396\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://tv.naver.com/v/2297124\"\u003ehttps://tv.naver.com/v/2297124\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"About me"}]